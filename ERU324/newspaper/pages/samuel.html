<div>
    <p>
        À medida que a inteligência artificial (IA) se expande rapidamente, surge uma crescente
        preocupação sobre seu potencial para perpetuar e agravar as disparidades sociais. A IA está
        sendo cada vez mais empregada em processos cruciais de tomada de decisão, como seleção de
        candidatos a emprego, concessão de empréstimos e até diagnósticos médicos. No entanto,
        recentemente, casos têm demonstrado como a IA pode ser influenciada pelos preconceitos presentes
        nos bancos de dados, contribuindo para a estratificação social e intensificando a desigualdade.
    </p>

    <figure class="figure first-image" style="margin-right: 1rem;">
        <img class="media fullmedia" src="/public/bulimia.jpg" alt="">
        <figcaption class="figcaption">The Bulimia Project. 2023. Disponível
            em:
            <a href="https://bulimia.com/examine/scrolling-into-bias/"
                target="_blank">https://bulimia.com/examine/scrolling-into-bias/</a>.
            Acesso em:
            02 abr. 2024.
        </figcaption>
    </figure>

    <p>
        Um exemplo recente ocorreu quando o The Bulimia Project testou geradores de imagens de
        IA para revelar o conceito de um físico "perfeito". Os resultados mostraram uma
        tendência alarmante: imagens predominantemente representavam mulheres loiras de pele
        branca e homens com características estereotipadas, como músculos superdefinidos. Essa
        reprodução de estereótipos reflete a falta de diversidade nos conjuntos de dados
        utilizados para treinar os algoritmos de IA.
    </p>
    <p>
        O professor Moacir Ponti, do Instituto de Ciências Matemáticas e de Computação de São
        Carlos (ICMC) da USP, destaca que o problema reside no desenvolvimento dessas IA por
        indivíduos sem consciência das desigualdades presentes nos dados e no uso desses
        sistemas por usuários sem conhecimento de sua origem. Ponti exemplifica como algoritmos
        de seleção de emprego podem perpetuar padrões desiguais, favorecendo certos perfis em
        detrimento de outros.
    </p>
    <p>
        Um caso emblemático foi observado na Amazon, onde uma ferramenta de IA discriminou
        candidatas do sexo feminino, penalizando-as por menções ao gênero em seus currículos.
        Essa discriminação reflete a falta de diversidade nos dados utilizados para treinar o
        algoritmo, reforçando padrões históricos de contratação desigual.
    </p>
    <p>
        Além disso, injustiças raciais também são evidentes no uso de IA, especialmente no
        sistema judiciário. A professora Lívia Oliveira, de Ciência da Computação, ressalta como
        a IA pode ser mais rígida com pessoas negras, contribuindo para o encarceramento
        desproporcional desses grupos.
    </p>

    <p>
        Diante desses desafios éticos, os programadores têm uma responsabilidade crucial. É
        fundamental testar, reavaliar e compreender o impacto dos algoritmos, garantindo um
        desenvolvimento responsável da IA. Conforme observado por Lívia Oliveira, entender os
        dados e os efeitos de suas decisões é essencial para garantir que a IA contribua para um
        futuro mais justo e igualitário.
    </p>
</div>